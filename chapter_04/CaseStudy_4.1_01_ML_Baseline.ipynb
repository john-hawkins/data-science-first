{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d25956-5dae-4d8d-a062-6235bd01e151",
   "metadata": {},
   "source": [
    "# 01 - ML Baseline Models (No listing in the book)\n",
    "\n",
    "This script trains a set of baseline models using our simple features.\n",
    "This listing was not shown in the book, just described and then shown as tabulated results.\n",
    "\n",
    "The purpose of this is not that we expect the best performance, but that we can understand how far a simple solution will take us.\n",
    "\n",
    "The corresponding python script for this notebook is:\n",
    "* [CaseStudy_4.1_01-03.py Baselines](CaseStudy_4.1_01-03.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858cb1c-83d1-44c8-828a-f337530cf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# BASELINE ML MODELS\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# SUPPORT MODULES\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfba6fe-aa7e-4b0e-b5b5-df6566832ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/complete_with_features.csv\")\n",
    "\n",
    "features = ['text_len', 'text_wc', 'text_sc', 'text_lc', 'text_avg_wl',\n",
    "            'text_max_wl', 'text_cwd', 'text_caps', 'text_punc',\n",
    "            'text_misspelling', 'text_grammar_err']\n",
    "\n",
    "train = df[df[\"RANDOM\"]<0.8]\n",
    "test = df[df[\"RANDOM\"]>=0.8]\n",
    "\n",
    "X_train = train.loc[:,features]\n",
    "y_train = train.loc[:,\"generated\"]\n",
    "X_test = test.loc[:,features]\n",
    "y_test = test.loc[:,\"generated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da96089-15d6-45fb-8335-8ee728656d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_pipeline(\n",
    "   StandardScaler()\n",
    ")\n",
    "\n",
    "nb = ComplementNB()\n",
    "lr = LogisticRegression(random_state=0)\n",
    "xt = ExtraTreesClassifier()\n",
    "\n",
    "lr_model = Pipeline(steps=[\n",
    "   ('preprocessor', preprocessor),\n",
    "   ('lr', lr )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d433c-f387-446c-a980-e4255353f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)\n",
    "xt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb8612-2b11-4716-b322-8123f24fc520",
   "metadata": {},
   "source": [
    "## Compile a results dataset\n",
    "\n",
    "Uses the models to score test data and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eea13-89e8-4a12-84d1-93f60f7bc921",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Model\", \"AUC\", \"Precision\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212d4b1-b8d1-4cff-b969-615c2f84b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the Naive Bayes Model\n",
    "y_pred = nb.predict(X_test)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "temp2 = nb.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, temp2[:,1])\n",
    "record = {\"Model\":\"NaiveBayes\", \"AUC\": auc, \"Precision\":prec, \"Recall\":recall}\n",
    "results = pd.concat([results, pd.DataFrame([record])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8afa8-8cd9-414a-ae33-7b1c069967de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the Logistic Regression Model\n",
    "y_pred = lr_model.predict(X_test)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "temp2 = lr_model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, temp2[:,1])\n",
    "record = {\"Model\":\"Logistic Regression\", \"AUC\": auc, \"Precision\":prec, \"Recall\":recall}\n",
    "results = pd.concat([results, pd.DataFrame([record])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05dafc5-2998-4fe3-b082-b9cf592cb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the Extra Trees Model\n",
    "y_pred = xt.predict(X_test)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "temp2 = xt.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, temp2[:,1])\n",
    "record = {\"Model\":\"Extra Trees\", \"AUC\": auc, \"Precision\":prec, \"Recall\":recall}\n",
    "results = pd.concat([results, pd.DataFrame([record])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694747cc-eec9-42b0-8820-6c292344d7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model               |   AUC |   Precision |   Recall |\n",
      "|:--------------------|------:|------------:|---------:|\n",
      "| NaiveBayes          | 0.911 |       0.747 |    0.855 |\n",
      "| Logistic Regression | 0.956 |       0.88  |    0.853 |\n",
      "| Extra Trees         | 0.99  |       0.956 |    0.924 |\n"
     ]
    }
   ],
   "source": [
    "results = results.round(3)\n",
    "# Display Results DataFrame as a Markdown Table\n",
    "markdown_table = results.to_markdown(index=False)\n",
    "print(markdown_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
