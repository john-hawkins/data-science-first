{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707696a3-cfd9-4f33-9d2c-d6e95413420f",
   "metadata": {},
   "source": [
    "# 03 - Machine Learning Models with Language Model Embeddings\n",
    "\n",
    "In this notebook we re-train our best machine learning model using the semantic embeddings as the features\n",
    "\n",
    "Note: This notebook assumes you have run all the scripts that generate the embeddings and you have saved the files in the same directory as this notebook. If you are using cloud compute you will need to download the embeddings files your generated on the server.\n",
    "\n",
    "The scripts to generate the embeddings are the following:\n",
    "* [CaseStudy_4.1_03-04_BERT.py](CaseStudy_4.1_03-04_BERT.py)\t\t\t\t\n",
    "* [CaseStudy_4.1_03-04b_CLS_TOKEN.py](CaseStudy_4.1_03-04b_CLS_TOKEN.py)\t\t\t\n",
    "* [CaseStudy_4.1_03-04c_SENTENCE_TRANSFORMERS.py](CaseStudy_4.1_03-04c_SENTENCE_TRANSFORMERS.py)\n",
    "* [CaseStudy_4.1_03-04d_INSTRUCT.py](CaseStudy_4.1_03-04d_INSTRUCT.py)\n",
    "\n",
    "The scripts to execute the model training are the following:\n",
    "* [Train and Evaluate - CaseStudy_4.1_03-05.py](CaseStudy_4.1_03-05.py)\n",
    "* [Train Best and Save - CaseStudy_4.1_03-06.py](CaseStudy_4.1_03-06.py)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960c5de-d729-4e9a-93f8-db9897a9fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# SUPPORT MODULES\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63caa054-87f6-42da-9d74-f18a825dfd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/complete_with_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b16007-ace4-49b3-b2f5-db7503c3ae11",
   "metadata": {},
   "source": [
    "Create a list of the embeddings we generated and want to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb9bcb-6319-401d-9117-e5d4430c7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {\n",
    "    \"BERT-Mean\":\"Embeddings_bert-base-uncased.npy\",\n",
    "    \"BERT-CLS\":\"Embeddings_CLS_bert-base-uncased.npy\",\n",
    "    \"TaylorAI\":\"Embeddings_TaylorAI.npy\",\n",
    "    \"MiniLm\":\"Embeddings_MiniLm.npy\",\n",
    "    \"Instruct\":\"Embeddings_instruct.npy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d6486-4b46-4f8a-9eff-3fa94493f9c2",
   "metadata": {},
   "source": [
    "A simple function that just makes each dimension of the embedding into a column and separate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303e4b3-6832-40cd-a938-c543aa864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_array_col(df, col_name):\n",
    "    expanded = df[col_name].apply(pd.Series)\n",
    "    expanded.columns = [f'{col_name}_{i+1}' for i in range(expanded.shape[1])]\n",
    "    df_expanded = pd.concat([df.drop(col_name, axis=1), expanded], axis=1)\n",
    "    return df_expanded, expanded.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9857c-4aaf-4759-9c9e-82f73d2f341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Embedding\", \"Length\", \"AUC\", \"Precision\", \"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784b39a-7cd0-4d37-87e2-1580bb3e1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in embeddings.keys():\n",
    "    print(f\"Loading : {k}\")\n",
    "    embs = np.load(embeddings[k], allow_pickle=True,)\n",
    "    df[\"embedding\"] = list(embs)\n",
    "    newdf, cols = expand_array_col(df, \"embedding\")\n",
    "    emb_len = len(cols)\n",
    "    train = newdf[newdf[\"RANDOM\"]<0.8]\n",
    "    test = newdf[newdf[\"RANDOM\"]>=0.8]\n",
    "    X_train = train.loc[:,cols]\n",
    "    y_train = train.loc[:,\"generated\"]\n",
    "    X_test = test.loc[:,cols]\n",
    "    y_test = test.loc[:,\"generated\"]\n",
    "\n",
    "    xt = ExtraTreesClassifier()\n",
    "    xt.fit(X_train, y_train)\n",
    "\n",
    "    # Metrics for the Extra Trees Model\n",
    "    y_pred = xt.predict(X_test)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    temp2 = xt.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, temp2[:,1])\n",
    "    record = {\"Embedding\":k,\"Length\":emb_len, \"AUC\": auc, \"Precision\":prec, \"Recall\":recall}\n",
    "    print(record)\n",
    "    results = pd.concat([results, pd.DataFrame([record])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01cbd9c-d7a5-4c9b-921e-f68c9cdb1b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Embedding   |   Length |   AUC |   Precision |   Recall |\n",
      "|:------------|---------:|------:|------------:|---------:|\n",
      "| BERT-Mean   |      768 | 0.999 |       0.993 |    0.984 |\n",
      "| BERT-CLS    |      768 | 0.998 |       0.986 |    0.975 |\n",
      "| TaylorAI    |      384 | 0.994 |       0.962 |    0.946 |\n",
      "| MiniLm      |      384 | 0.989 |       0.945 |    0.933 |\n",
      "| Instruct    |     1024 | 0.999 |       0.993 |    0.978 |\n"
     ]
    }
   ],
   "source": [
    "# Round all numbers before displaying\n",
    "results = results.round(3)\n",
    "# Display Results DataFrame as a Markdown Table\n",
    "markdown_table = results.to_markdown(index=False)\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858adab6-bec9-4f22-af24-41ff5a584117",
   "metadata": {},
   "source": [
    "## Serialise the best model\n",
    "\n",
    "After this evaliation we created a script to retrain the best model and save it to a pickle file so it could be loaded and reused in later experiments. This is all done with the script:\n",
    "\n",
    "* [Train Best and Save - CaseStudy_4.1_03-06.py](CaseStudy_4.1_03-06.py)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a27af-671e-4b76-88a6-30bd24756a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
