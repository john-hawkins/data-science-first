{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4905b8a-c194-42be-86c1-69ed829d2c6f",
   "metadata": {},
   "source": [
    "# Listing 01 - Load, join, clean data\n",
    "\n",
    "You will need to download all of the datasets discussed in the book into your local data directory.\n",
    "\n",
    "The corresponding python scripts for this notebook are:\n",
    "* [CaseStudy_4.1_00-01.py Data Prep](CaseStudy_4.1_00-01.py)\n",
    "* [CaseStudy_4.1_00-02.py Summarization](CaseStudy_4.1_00-02.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5fa6e-4e7e-440c-9df0-478df191c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a68caf-908e-45a4-94bd-2924355ce6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/llm-detect-ai-generated-text/train_essays.csv\")\n",
    "df2 = pd.read_csv(\"data/train_v2_drcat_02.csv\")\n",
    "df3 = pd.read_csv(\"data/Training_Essay_Data.csv\")\n",
    "\n",
    "df1[\"source\"] = \"LA-Lab\"\n",
    "df2[\"source\"] = \"Darek\"\n",
    "df3[\"source\"] = \"Sunil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b98b88-573b-4f4f-af28-cac965c67094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all data sets have the same column for the target\n",
    "df2[\"generated\"] = df2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b31f75-61c1-449e-b7d3-2c3b8d085a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"source\", \"text\", \"generated\"]\n",
    "df1 = df1.loc[:,cols]\n",
    "df2 = df2.loc[:,cols]\n",
    "df3 = df3.loc[:,cols]\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "records = len(df)\n",
    "\n",
    "print(f\"Joined dataset contains {records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66bb99-2c62-41b7-af88-00791151ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df.drop_duplicates(subset=['text'], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "new_records = len(df)\n",
    "print(\"Dropped\", records-new_records, \"Records\")\n",
    "\n",
    "df['RANDOM'] = df.apply(lambda x: random.random(), axis=1)\n",
    "df.to_csv(\"data/complete_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd660d5-962e-41af-90bb-3455cdd5dc9c",
   "metadata": {},
   "source": [
    "# Listing 02 - Summarization\n",
    "\n",
    "Quick analysis to understand the dataset content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0454a-7100-4cd0-b394-c970287a064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8de35-9969-49ef-a9ff-771f421bfc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"chars\"] = df['text'].apply(len)\n",
    "\n",
    "def word_count(t):\n",
    "    wds = t.split(\" \")\n",
    "    return len(wds)\n",
    "\n",
    "df[\"words\"] = df['text'].apply(word_count)\n",
    "\n",
    "def word_len(t):\n",
    "    wds = t.split(\" \")\n",
    "    lens = [len(x) for x in wds]\n",
    "    return np.mean(lens)\n",
    "\n",
    "df[\"avg_wd\"] = df.apply(lambda x: word_len(x['text']), axis='columns')\n",
    "\n",
    "df[\"creator\"] = np.where(df['generated']==1,\"GenAI\",\"Human\")\n",
    "\n",
    "summary = df.groupby([\"source\",\"creator\"]).agg({\"generated\":\"count\",\"chars\":\"mean\", \"words\":\"mean\",\"avg_wd\":\"mean\"}).reset_index()\n",
    "summary = summary.round(1)\n",
    "\n",
    "summary.columns = [\"Data\", \"Origin\", \"Records\", \"Avg Chrs\", \"Avg Wds\", \"Avg Wd Len\"]\n",
    "\n",
    "\n",
    "# Display DataFrame as a Markdown Table\n",
    "markdown_table = summary.to_markdown(index=False)\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ac10c-1638-4bdb-af8e-52183a3d2632",
   "metadata": {},
   "source": [
    "# Listing 03 - Feature Engineering\n",
    "\n",
    "We do very simple feature engineering for our baseline model using a python command line application that calculates text statistics and adds them as new columns. We install this library and then execute the script as a simple BASH command in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760e9b9-1df3-498f-a353-7c9f39d88419",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install texturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a277c6-c906-4165-941c-5f991a3f64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# BASH SCRIPT TO PROCESS DATA WITH SIMPLE FEATURES USING TEXTURIZER\n",
    "\n",
    "input=data/complete_dataset.csv\n",
    "output=data/complete_with_features.csv\n",
    "texturizer -columns=text -literacy $input > $output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46a5b9-546d-43ad-b5d3-cb16cc915ece",
   "metadata": {},
   "source": [
    "Take a look at the content of the file we just generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c814573-8c3a-4a4f-88cb-5639604bf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/complete_with_features.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
